{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44542104",
   "metadata": {},
   "source": [
    "# CNN for Saliency Center Detection\n",
    "\n",
    "This notebook implements a Convolutional Neural Network that detects the \"saliency center\" pixel of an image. The model takes RGB + Depth images (256×256×4) as input and outputs UV coordinates (u, v) of the center of the most salient object in the image.\n",
    "\n",
    "## Table of Contents\n",
    "1. Import Required Libraries\n",
    "2. Model Architecture\n",
    "3. Data Generation and Preprocessing\n",
    "4. Training Setup\n",
    "5. Model Training\n",
    "6. Evaluation and Visualization\n",
    "7. Testing and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901ce8b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for building our saliency center detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac486655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install required packages\n",
    "packages = [\"tensorflow\", \"numpy\", \"matplotlib\", \"opencv-python\", \"scikit-learn\", \"pillow\"]\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983f4dc",
   "metadata": {},
   "source": [
    "## 2. Model Architecture\n",
    "\n",
    "Let's build the CNN model for detecting the saliency center. The model takes 256×256×4 images (RGB + Depth) and outputs normalized UV coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e759152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for detecting the \"saliency center\" pixel of an image.\n",
    "# It should return the uv coordinates of the center of the most salient object in the image.\n",
    "\n",
    "def create_saliency_center_model(input_shape=(256, 256, 4)):\n",
    "    \"\"\"\n",
    "    Create a CNN model for saliency center detection.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Shape of input images (height, width, channels)\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input: 256×256×4 (RGB + Depth)\n",
    "        Input(shape=input_shape),\n",
    "\n",
    "        # Convolutional blocks - extract features\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),  # 128×128\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),  # 64×64\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),  # 32×32\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),  # 16×16\n",
    "\n",
    "        # Flatten and dense layers\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Output: 2 values (u, v coordinates) constrained to range [0, 1] in u and v\n",
    "        Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "saliency_center_model = create_saliency_center_model()\n",
    "\n",
    "# Compile for regression\n",
    "saliency_center_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',  # Mean Squared Error for coordinate prediction\n",
    "    metrics=['mae']  # Mean Absolute Error for monitoring\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"Saliency Center Detection Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "saliency_center_model.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = saliency_center_model.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230a4ff",
   "metadata": {},
   "source": [
    "## 3. Data Generation and Preprocessing\n",
    "\n",
    "Since we don't have a real dataset, let's create synthetic data for demonstration purposes. In a real scenario, you would load your actual RGB+D images and corresponding saliency center annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples=1000, img_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Generate synthetic RGB+D images with known saliency centers for training.\n",
    "\n",
    "    Args:\n",
    "        num_samples: Number of samples to generate\n",
    "        img_size: Size of images (height, width)\n",
    "\n",
    "    Returns:\n",
    "        X: Array of shape (num_samples, height, width, 4) - RGB+D images\n",
    "        y: Array of shape (num_samples, 2) - Normalized UV coordinates\n",
    "    \"\"\"\n",
    "    height, width = img_size\n",
    "    X = np.zeros((num_samples, height, width, 4), dtype=np.float32)\n",
    "    y = np.zeros((num_samples, 2), dtype=np.float32)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Create base image with noise\n",
    "        rgb_img = np.random.uniform(0, 0.3, (height, width, 3))\n",
    "        depth_img = np.random.uniform(0, 0.3, (height, width, 1))\n",
    "\n",
    "        # Create salient object (bright circle or rectangle)\n",
    "        center_u = np.random.uniform(0.2, 0.8)  # Avoid edges\n",
    "        center_v = np.random.uniform(0.2, 0.8)\n",
    "\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        center_x = int(center_u * width)\n",
    "        center_y = int(center_v * height)\n",
    "\n",
    "        # Create salient object\n",
    "        object_type = np.random.choice(['circle', 'rectangle'])\n",
    "\n",
    "        if object_type == 'circle':\n",
    "            radius = np.random.randint(20, 60)\n",
    "            y_grid, x_grid = np.ogrid[:height, :width]\n",
    "            mask = (x_grid - center_x)**2 + (y_grid - center_y)**2 <= radius**2\n",
    "\n",
    "            # Bright colored circle\n",
    "            color = np.random.uniform(0.7, 1.0, 3)\n",
    "            for c in range(3):\n",
    "                rgb_img[mask, c] = color[c]\n",
    "            depth_img[mask, 0] = np.random.uniform(0.8, 1.0)\n",
    "\n",
    "        else:  # rectangle\n",
    "            size = np.random.randint(30, 80)\n",
    "            x1 = max(0, center_x - size//2)\n",
    "            x2 = min(width, center_x + size//2)\n",
    "            y1 = max(0, center_y - size//2)\n",
    "            y2 = min(height, center_y + size//2)\n",
    "\n",
    "            # Bright colored rectangle\n",
    "            color = np.random.uniform(0.7, 1.0, 3)\n",
    "            rgb_img[y1:y2, x1:x2] = color.reshape(1, 1, 3)\n",
    "            depth_img[y1:y2, x1:x2, 0] = np.random.uniform(0.8, 1.0)\n",
    "\n",
    "        # Combine RGB and Depth\n",
    "        X[i] = np.concatenate([rgb_img, depth_img], axis=2)\n",
    "        y[i] = [center_u, center_v]  # Normalized coordinates\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Generate synthetic dataset\n",
    "print(\"Generating synthetic dataset...\")\n",
    "num_samples = 2000\n",
    "X_data, y_data = generate_synthetic_data(num_samples)\n",
    "\n",
    "print(f\"Generated {num_samples} samples\")\n",
    "print(f\"Image shape: {X_data.shape}\")\n",
    "print(f\"Coordinates shape: {y_data.shape}\")\n",
    "print(f\"Coordinate ranges - U: [{y_data[:, 0].min():.3f}, {y_data[:, 0].max():.3f}]\")\n",
    "print(f\"Coordinate ranges - V: [{y_data[:, 1].min():.3f}, {y_data[:, 1].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec40c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample data\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Sample Synthetic Data', fontsize=16)\n",
    "\n",
    "for i in range(4):\n",
    "    # RGB visualization\n",
    "    axes[0, i].imshow(X_data[i, :, :, :3])  # RGB channels\n",
    "    axes[0, i].set_title(f'RGB - Sample {i+1}')\n",
    "    axes[0, i].set_xticks([])\n",
    "    axes[0, i].set_yticks([])\n",
    "\n",
    "    # Mark the true saliency center\n",
    "    u, v = y_data[i]\n",
    "    center_x = u * 256\n",
    "    center_y = v * 256\n",
    "    axes[0, i].plot(center_x, center_y, 'r+', markersize=15, markeredgewidth=3)\n",
    "    axes[0, i].text(10, 30, f'UV: ({u:.2f}, {v:.2f})', color='red',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # Depth visualization\n",
    "    axes[1, i].imshow(X_data[i, :, :, 3], cmap='gray')  # Depth channel\n",
    "    axes[1, i].set_title(f'Depth - Sample {i+1}')\n",
    "    axes[1, i].set_xticks([])\n",
    "    axes[1, i].set_yticks([])\n",
    "    axes[1, i].plot(center_x, center_y, 'r+', markersize=15, markeredgewidth=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffc78d",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "Let's split our data into training and validation sets and set up training callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24038ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "\n",
    "# Define training callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'saliency_center_model_best.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Max epochs: {epochs}\")\n",
    "print(f\"  Callbacks: Early stopping, Learning rate reduction, Model checkpointing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661fb9af",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Now let's train our saliency center detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "history = saliency_center_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Print final metrics\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_mae = history.history['mae'][-1]\n",
    "final_val_mae = history.history['val_mae'][-1]\n",
    "\n",
    "print(f\"Final Training Loss (MSE): {final_train_loss:.6f}\")\n",
    "print(f\"Final Validation Loss (MSE): {final_val_loss:.6f}\")\n",
    "print(f\"Final Training MAE: {final_train_mae:.6f}\")\n",
    "print(f\"Final Validation MAE: {final_val_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2db429",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Visualization\n",
    "\n",
    "Let's visualize the training progress and evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a08226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_title('Model Loss (MSE)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE plot\n",
    "ax2.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "ax2.set_title('Model Mean Absolute Error', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate pixel-level error statistics\n",
    "def calculate_pixel_errors(y_true, y_pred, img_size=(256, 256)):\n",
    "    \"\"\"Calculate pixel-level errors from normalized coordinates\"\"\"\n",
    "    # Convert normalized coordinates to pixel coordinates\n",
    "    pixel_true = y_true * np.array([img_size[1], img_size[0]])  # [width, height]\n",
    "    pixel_pred = y_pred * np.array([img_size[1], img_size[0]])\n",
    "\n",
    "    # Calculate Euclidean distance in pixels\n",
    "    pixel_errors = np.sqrt(np.sum((pixel_true - pixel_pred)**2, axis=1))\n",
    "\n",
    "    return pixel_errors, pixel_true, pixel_pred\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions = saliency_center_model.predict(X_val, verbose=0)\n",
    "pixel_errors, pixel_true, pixel_pred = calculate_pixel_errors(y_val, val_predictions)\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Mean Squared Error (normalized): {mean_squared_error(y_val, val_predictions):.6f}\")\n",
    "print(f\"Mean Absolute Error (normalized): {mean_absolute_error(y_val, val_predictions):.6f}\")\n",
    "print(f\"Mean pixel error: {np.mean(pixel_errors):.2f} pixels\")\n",
    "print(f\"Median pixel error: {np.median(pixel_errors):.2f} pixels\")\n",
    "print(f\"95th percentile pixel error: {np.percentile(pixel_errors, 95):.2f} pixels\")\n",
    "print(f\"Max pixel error: {np.max(pixel_errors):.2f} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction results\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Saliency Center Detection Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(4):\n",
    "    idx = i * 10  # Sample different images\n",
    "\n",
    "    # Original RGB image\n",
    "    axes[0, i].imshow(X_val[idx, :, :, :3])\n",
    "    axes[0, i].set_title(f'RGB Image {idx+1}')\n",
    "    axes[0, i].set_xticks([])\n",
    "    axes[0, i].set_yticks([])\n",
    "\n",
    "    # Mark true and predicted centers\n",
    "    true_u, true_v = y_val[idx]\n",
    "    pred_u, pred_v = val_predictions[idx]\n",
    "\n",
    "    true_x, true_y = true_u * 256, true_v * 256\n",
    "    pred_x, pred_y = pred_u * 256, pred_v * 256\n",
    "\n",
    "    axes[0, i].plot(true_x, true_y, 'g+', markersize=15, markeredgewidth=3, label='True')\n",
    "    axes[0, i].plot(pred_x, pred_y, 'r+', markersize=15, markeredgewidth=3, label='Predicted')\n",
    "    if i == 0:\n",
    "        axes[0, i].legend()\n",
    "\n",
    "    # Depth image\n",
    "    axes[1, i].imshow(X_val[idx, :, :, 3], cmap='gray')\n",
    "    axes[1, i].set_title(f'Depth Image {idx+1}')\n",
    "    axes[1, i].set_xticks([])\n",
    "    axes[1, i].set_yticks([])\n",
    "    axes[1, i].plot(true_x, true_y, 'g+', markersize=15, markeredgewidth=3)\n",
    "    axes[1, i].plot(pred_x, pred_y, 'r+', markersize=15, markeredgewidth=3)\n",
    "\n",
    "    # Error visualization\n",
    "    error_map = np.zeros((256, 256, 3))\n",
    "    error_map[:, :, 0] = X_val[idx, :, :, 0]  # Use red channel as base\n",
    "\n",
    "    # Draw error line\n",
    "    cv2.line(error_map,\n",
    "            (int(true_x), int(true_y)),\n",
    "            (int(pred_x), int(pred_y)),\n",
    "            (1, 1, 0), 2)  # Yellow line\n",
    "\n",
    "    axes[2, i].imshow(error_map)\n",
    "    axes[2, i].set_title(f'Error: {pixel_errors[idx]:.1f} pixels')\n",
    "    axes[2, i].set_xticks([])\n",
    "    axes[2, i].set_yticks([])\n",
    "\n",
    "    # Add coordinate text\n",
    "    axes[2, i].text(10, 30, f'True: ({true_u:.2f}, {true_v:.2f})',\n",
    "                   color='green', fontweight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    axes[2, i].text(10, 60, f'Pred: ({pred_u:.2f}, {pred_v:.2f})',\n",
    "                   color='red', fontweight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6d3ac",
   "metadata": {},
   "source": [
    "## 7. Testing and Inference\n",
    "\n",
    "Let's test the model with new synthetic data and demonstrate how to use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ba985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "print(\"Generating test data...\")\n",
    "X_test, y_test = generate_synthetic_data(200)\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Test the model\n",
    "test_predictions = saliency_center_model.predict(X_test, verbose=0)\n",
    "test_pixel_errors, test_pixel_true, test_pixel_pred = calculate_pixel_errors(y_test, test_predictions)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Mean Squared Error (normalized): {mean_squared_error(y_test, test_predictions):.6f}\")\n",
    "print(f\"Mean Absolute Error (normalized): {mean_absolute_error(y_test, test_predictions):.6f}\")\n",
    "print(f\"Mean pixel error: {np.mean(test_pixel_errors):.2f} pixels\")\n",
    "print(f\"Median pixel error: {np.median(test_pixel_errors):.2f} pixels\")\n",
    "print(f\"95th percentile pixel error: {np.percentile(test_pixel_errors, 95):.2f} pixels\")\n",
    "\n",
    "# Function for inference on a single image\n",
    "def predict_saliency_center(model, image):\n",
    "    \"\"\"\n",
    "    Predict saliency center for a single image.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        image: Input image of shape (height, width, 4) - RGB+D\n",
    "\n",
    "    Returns:\n",
    "        u, v: Normalized coordinates of saliency center\n",
    "        pixel_x, pixel_y: Pixel coordinates of saliency center\n",
    "    \"\"\"\n",
    "    # Ensure image is the right shape\n",
    "    if len(image.shape) == 3:\n",
    "        image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(image, verbose=0)[0]\n",
    "    u, v = prediction\n",
    "\n",
    "    # Convert to pixel coordinates\n",
    "    height, width = image.shape[1:3]\n",
    "    pixel_x = u * width\n",
    "    pixel_y = v * height\n",
    "\n",
    "    return u, v, pixel_x, pixel_y\n",
    "\n",
    "# Demonstrate inference on a few test examples\n",
    "print(\"\\nInference Examples:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i in range(3):\n",
    "    # Select a test image\n",
    "    test_img = X_test[i]\n",
    "    true_u, true_v = y_test[i]\n",
    "\n",
    "    # Predict saliency center\n",
    "    pred_u, pred_v, pred_x, pred_y = predict_saliency_center(saliency_center_model, test_img)\n",
    "\n",
    "    # Calculate error\n",
    "    pixel_error = np.sqrt((true_u*256 - pred_x)**2 + (true_v*256 - pred_y)**2)\n",
    "\n",
    "    print(f\"Image {i+1}:\")\n",
    "    print(f\"  True center (UV): ({true_u:.3f}, {true_v:.3f})\")\n",
    "    print(f\"  Predicted center (UV): ({pred_u:.3f}, {pred_v:.3f})\")\n",
    "    print(f\"  Predicted center (pixels): ({pred_x:.1f}, {pred_y:.1f})\")\n",
    "    print(f\"  Pixel error: {pixel_error:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Histogram of pixel errors\n",
    "ax1.hist(test_pixel_errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(np.mean(test_pixel_errors), color='red', linestyle='--',\n",
    "           label=f'Mean: {np.mean(test_pixel_errors):.2f}')\n",
    "ax1.axvline(np.median(test_pixel_errors), color='green', linestyle='--',\n",
    "           label=f'Median: {np.median(test_pixel_errors):.2f}')\n",
    "ax1.set_xlabel('Pixel Error')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Pixel Errors', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot of predicted vs true coordinates\n",
    "ax2.scatter(y_test[:, 0], test_predictions[:, 0], alpha=0.6, label='U coordinate')\n",
    "ax2.scatter(y_test[:, 1], test_predictions[:, 1], alpha=0.6, label='V coordinate')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect prediction')\n",
    "ax2.set_xlabel('True Coordinates')\n",
    "ax2.set_ylabel('Predicted Coordinates')\n",
    "ax2.set_title('Predicted vs True Coordinates', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final model summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SALIENCY CENTER DETECTION MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model Architecture: CNN with 4 conv blocks + 2 dense layers\")\n",
    "print(f\"Input Shape: (256, 256, 4) - RGB + Depth\")\n",
    "print(f\"Output Shape: (2,) - Normalized UV coordinates [0,1]\")\n",
    "print(f\"Total Parameters: {saliency_center_model.count_params():,}\")\n",
    "print(f\"Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation Samples: {X_val.shape[0]}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]}\")\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Mean Pixel Error: {np.mean(test_pixel_errors):.2f} pixels\")\n",
    "print(f\"  Median Pixel Error: {np.median(test_pixel_errors):.2f} pixels\")\n",
    "print(f\"  Success Rate (< 20 pixels): {np.sum(test_pixel_errors < 20) / len(test_pixel_errors) * 100:.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805708c",
   "metadata": {},
   "source": [
    "## Usage Notes and Next Steps\n",
    "\n",
    "### How to Use This Model:\n",
    "\n",
    "1. **Input Format**: The model expects 256×256×4 images (RGB + Depth channels)\n",
    "2. **Output Format**: Returns normalized UV coordinates [0,1] representing the saliency center\n",
    "3. **Preprocessing**: Ensure input images are normalized to [0,1] range\n",
    "\n",
    "### For Real Applications:\n",
    "\n",
    "1. **Replace Synthetic Data**: Use your actual RGB+D dataset with manually annotated saliency centers\n",
    "2. **Data Augmentation**: Add rotation, scaling, and color jittering to improve generalization\n",
    "3. **Transfer Learning**: Pre-train on synthetic data, then fine-tune on real data\n",
    "4. **Multi-scale Training**: Train on different image resolutions for robustness\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "- **Attention Mechanisms**: Add spatial attention layers to focus on salient regions\n",
    "- **Multi-task Learning**: Predict both center and saliency maps simultaneously  \n",
    "- **Uncertainty Estimation**: Add prediction confidence/uncertainty quantification\n",
    "- **Real-time Optimization**: Model quantization and pruning for deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
